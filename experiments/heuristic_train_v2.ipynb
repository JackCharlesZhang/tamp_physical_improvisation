{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom DQN Training for Distance Heuristic\n",
    "\n",
    "This notebook tests the custom DQN implementation on a tiny gridworld environment.\n",
    "\n",
    "## Goals\n",
    "1. Collect training data (state pairs from planning graph)\n",
    "2. Train distance heuristic using transparent custom DQN\n",
    "3. Evaluate learned distances vs true distances\n",
    "4. Debug any issues with full visibility into training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path if needed\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "from tamp_improv.benchmarks.gridworld import GridworldTAMPSystem\n",
    "from tamp_improv.approaches.improvisational.base import ImprovisationalTAMPApproach\n",
    "from tamp_improv.approaches.improvisational.policies.multi_rl import MultiRLPolicy\n",
    "from tamp_improv.approaches.improvisational.policies.rl import RLConfig\n",
    "from tamp_improv.approaches.improvisational.collection import collect_total_shortcuts\n",
    "from tamp_improv.approaches.improvisational.distance_heuristic_v2 import (\n",
    "    GoalConditionedDistanceHeuristicV2,\n",
    "    DistanceHeuristicConfig,\n",
    "    compute_reward\n",
    ")\n",
    "from tamp_improv.approaches.improvisational.graph_training import compute_graph_distances\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Environment: 2x2 cells, 5x5 states/cell\n",
      "  Total grid size: 10x10\n",
      "  Teleporters: 0\n",
      "  Collection: 10 episode(s)\n",
      "  Training: 500000 steps\n"
     ]
    }
   ],
   "source": [
    "# Environment configuration\n",
    "SEED = 42\n",
    "NUM_CELLS = 2  # 2x2 grid of cells\n",
    "NUM_STATES_PER_CELL = 5  # 5x5 states per cell\n",
    "NUM_TELEPORTERS = 0  # 1 portal pair\n",
    "\n",
    "# Collection configuration\n",
    "COLLECT_EPISODES = 10  # Very small for testing\n",
    "\n",
    "# Training configuration\n",
    "TRAINING_STEPS = 500000  # Short training for testing\n",
    "MAX_EPISODE_STEPS = 50  # Max steps per episode\n",
    "LEARNING_STARTS = 500  # Start learning after 500 steps\n",
    "LOG_FREQ = 500  # Log every 500 steps\n",
    "EVAL_FREQ = 2500  # Evaluate every 2500 steps\n",
    "TRAIN_FREQ = 1000 # update every 1000 steps\n",
    "TARGET_UPDATE_FREQ=2000\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Environment: {NUM_CELLS}x{NUM_CELLS} cells, {NUM_STATES_PER_CELL}x{NUM_STATES_PER_CELL} states/cell\")\n",
    "print(f\"  Total grid size: {NUM_CELLS * NUM_STATES_PER_CELL}x{NUM_CELLS * NUM_STATES_PER_CELL}\")\n",
    "print(f\"  Teleporters: {NUM_TELEPORTERS}\")\n",
    "print(f\"  Collection: {COLLECT_EPISODES} episode(s)\")\n",
    "print(f\"  Training: {TRAINING_STEPS} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Gridworld Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Action space is not Box, using original action space.\n",
      "✓ Created GridworldTAMPSystem\n",
      "  System name: TAMPSystem\n",
      "  Action space: Discrete(5)\n",
      "  Observation space: Graph(Box(-inf, inf, (6,), float32), None)\n"
     ]
    }
   ],
   "source": [
    "# Create tiny gridworld\n",
    "system = GridworldTAMPSystem.create_default(\n",
    "    num_cells=NUM_CELLS,\n",
    "    num_states_per_cell=NUM_STATES_PER_CELL,\n",
    "    num_teleporters=NUM_TELEPORTERS,\n",
    "    seed=SEED,\n",
    "    max_episode_steps=200,\n",
    ")\n",
    "\n",
    "print(f\"✓ Created GridworldTAMPSystem\")\n",
    "print(f\"  System name: {system.name}\")\n",
    "print(f\"  Action space: {system.env.action_space}\")\n",
    "print(f\"  Observation space: {system.env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Collect Training Data\n",
    "\n",
    "This builds the planning graph and collects state pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created approach\n"
     ]
    }
   ],
   "source": [
    "# Create approach for collection\n",
    "rl_config = RLConfig(device=device)\n",
    "policy = MultiRLPolicy(seed=SEED, config=rl_config)\n",
    "approach = ImprovisationalTAMPApproach(system, policy, seed=SEED)\n",
    "approach.training_mode = True\n",
    "\n",
    "print(\"✓ Created approach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting training data...\n",
      "\n",
      "================================================================================\n",
      "Collecting total planning graph from 10 episodes\n",
      "================================================================================\n",
      "Building total planning graph from 10 episodes...\n",
      "Planning graph with 5 nodes and 5 edges\n",
      "  Step 1/1: 0 -> 1\n",
      "  Step 1/1: 0 -> 2\n",
      "  Step 1/2: 0 -> 1\n",
      "  Step 2/2: 1 -> 3\n",
      "  Step 1/3: 0 -> 1\n",
      "  Step 2/3: 1 -> 3\n",
      "  Step 3/3: 3 -> 4\n",
      "Planning graph with 5 nodes and 5 edges\n",
      "  Step 1/1: 0 -> 1\n",
      "  Step 1/1: 0 -> 2\n",
      "  Step 1/2: 0 -> 1\n",
      "  Step 2/2: 1 -> 3\n",
      "  Step 1/3: 0 -> 1\n",
      "  Step 2/3: 1 -> 3\n",
      "  Step 3/3: 3 -> 4\n",
      "Planning graph with 5 nodes and 5 edges\n",
      "  Step 1/1: 0 -> 1\n",
      "  Step 1/1: 0 -> 2\n",
      "  Step 1/2: 0 -> 1\n",
      "  Step 2/2: 1 -> 3\n",
      "  Step 1/3: 0 -> 1\n",
      "  Step 2/3: 1 -> 3\n",
      "  Step 1/3: 0 -> 1\n",
      "  Step 2/3: 1 -> 3\n",
      "  Step 3/3: 3 -> 4\n",
      "Planning graph with 5 nodes and 5 edges\n",
      "  Step 1/1: 0 -> 1\n",
      "  Step 1/1: 0 -> 2\n",
      "  Step 1/2: 0 -> 1\n",
      "  Step 2/2: 1 -> 3\n",
      "  Step 1/2: 0 -> 1\n",
      "  Step 2/2: 1 -> 3\n",
      "  Step 1/2: 0 -> 1\n",
      "  Step 2/2: 1 -> 3\n",
      "  Step 1/3: 0 -> 1\n",
      "  Step 2/3: 1 -> 3\n",
      "  Step 3/3: 3 -> 4\n",
      "Planning graph with 5 nodes and 5 edges\n",
      "  Step 1/1: 0 -> 1\n",
      "  Step 1/1: 0 -> 2\n",
      "  Step 1/2: 0 -> 1\n",
      "  Step 2/2: 1 -> 3\n",
      "  Step 1/3: 0 -> 1\n",
      "  Step 2/3: 1 -> 3\n",
      "  Step 3/3: 3 -> 4\n",
      "  Episode 5/10: 5 nodes, 5 edges\n",
      "Planning graph with 5 nodes and 5 edges\n",
      "  Step 1/1: 0 -> 1\n",
      "  Step 1/1: 0 -> 2\n",
      "  Step 1/2: 0 -> 1\n",
      "  Step 2/2: 1 -> 3\n",
      "  Step 1/3: 0 -> 1\n",
      "  Step 2/3: 1 -> 3\n",
      "  Step 3/3: 3 -> 4\n",
      "Planning graph with 5 nodes and 5 edges\n",
      "  Step 1/1: 0 -> 1\n",
      "  Step 1/1: 0 -> 2\n",
      "  Step 1/2: 0 -> 1\n",
      "  Step 2/2: 1 -> 3\n",
      "  Step 1/3: 0 -> 1\n",
      "  Step 2/3: 1 -> 3\n",
      "  Step 3/3: 3 -> 4\n",
      "Planning graph with 5 nodes and 5 edges\n",
      "  Step 1/1: 0 -> 1\n",
      "  Step 1/1: 0 -> 2\n",
      "  Step 1/2: 0 -> 1\n",
      "  Step 2/2: 1 -> 3\n",
      "  Step 1/3: 0 -> 1\n",
      "  Step 2/3: 1 -> 3\n",
      "  Step 3/3: 3 -> 4\n",
      "Planning graph with 5 nodes and 5 edges\n",
      "  Step 1/1: 0 -> 1\n",
      "  Step 1/1: 0 -> 2\n",
      "  Step 1/2: 0 -> 1\n",
      "  Step 2/2: 1 -> 3\n",
      "  Step 1/3: 0 -> 1\n",
      "  Step 2/3: 1 -> 3\n",
      "  Step 3/3: 3 -> 4\n",
      "Planning graph with 5 nodes and 5 edges\n",
      "  Step 1/1: 0 -> 1\n",
      "  Step 1/1: 0 -> 2\n",
      "  Step 1/2: 0 -> 1\n",
      "  Step 2/2: 1 -> 3\n",
      "  Step 1/3: 0 -> 1\n",
      "  Step 2/3: 1 -> 3\n",
      "  Step 1/3: 0 -> 1\n",
      "  Step 2/3: 1 -> 3\n",
      "  Step 3/3: 3 -> 4\n",
      "  Episode 10/10: 5 nodes, 5 edges\n",
      "Total graph complete: 5 nodes, 5 edges, 49 total states\n",
      "\n",
      "Node ID -> Atoms mapping:\n",
      "================================================================================\n",
      "  Node   0 ( 0 states): (InCol0 robot0), (InRow0 robot0)\n",
      "  Node   1 ( 0 states): (InCol0 robot0), (InRow1 robot0)\n",
      "  Node   2 ( 0 states): (InCol1 robot0), (InRow0 robot0)\n",
      "  Node   3 ( 0 states): (InCol1 robot0), (InRow1 robot0)\n",
      "  Node   4 ( 0 states): (GoalReached robot0), (InCol1 robot0), (InRow1 robot0)\n",
      "================================================================================\n",
      "\n",
      "Computing edge costs using 25 samples per edge...\n",
      "Edge cost computation complete!\n",
      "\n",
      "================================================================================\n",
      "Identifying shortcuts from total graph\n",
      "================================================================================\n",
      "Identified 5 shortcut candidates\n",
      "\n",
      "================================================================================\n",
      "Collection complete: 50 training examples, 50 shortcuts\n",
      "================================================================================\n",
      "\n",
      "✓ Collection complete!\n",
      "  Nodes with states: 5\n",
      "  Total shortcuts: 50\n",
      "  Planning graph: 5 nodes, 5 edges\n"
     ]
    }
   ],
   "source": [
    "# Collection config\n",
    "config_dict = {\n",
    "    \"seed\": SEED,\n",
    "    \"collect_episodes\": COLLECT_EPISODES,\n",
    "    \"use_random_rollouts\": True,\n",
    "    \"num_rollouts_per_node\": 5,\n",
    "    \"max_steps_per_rollout\": 100,\n",
    "    \"shortcut_success_threshold\": 0.5,\n",
    "}\n",
    "\n",
    "# Collect data\n",
    "print(\"\\nCollecting training data...\")\n",
    "rng = np.random.default_rng(SEED)\n",
    "training_data = collect_total_shortcuts(system, approach, config_dict, rng=rng)\n",
    "\n",
    "print(f\"\\n✓ Collection complete!\")\n",
    "print(f\"  Nodes with states: {len(training_data.node_states)}\")\n",
    "print(f\"  Total shortcuts: {len(training_data.valid_shortcuts)}\")\n",
    "print(f\"  Planning graph: {len(training_data.graph.nodes)} nodes, {len(training_data.graph.edges)} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare State Pairs for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Prepared 1920 state pairs for training\n",
      "\n",
      "Sample state pairs:\n",
      "  Pair 0: 2 source atoms -> 2 target atoms\n",
      "  Pair 1: 2 source atoms -> 2 target atoms\n",
      "  Pair 2: 2 source atoms -> 2 target atoms\n"
     ]
    }
   ],
   "source": [
    "# Extract state pairs from training data\n",
    "planning_graph = training_data.graph\n",
    "all_node_states = training_data.node_states\n",
    "\n",
    "# Collect all (source_state, target_state) pairs\n",
    "state_pairs = []\n",
    "for source_node in planning_graph.nodes:\n",
    "    for target_node in planning_graph.nodes:\n",
    "        if source_node.id == target_node.id:\n",
    "            continue\n",
    "        \n",
    "        if source_node.id not in all_node_states or target_node.id not in all_node_states:\n",
    "            continue\n",
    "        \n",
    "        source_states = all_node_states[source_node.id]\n",
    "        target_states = all_node_states[target_node.id]\n",
    "        \n",
    "        if not source_states or not target_states:\n",
    "            continue\n",
    "        \n",
    "        # Take first state from each node (for simplicity)\n",
    "        # Could also take all combinations: for s in source_states for t in target_states\n",
    "        for i in range(len(source_states)):\n",
    "            for j in range(len(target_states)):\n",
    "                state_pairs.append((source_states[i], target_states[j]))\n",
    "\n",
    "print(f\"\\n✓ Prepared {len(state_pairs)} state pairs for training\")\n",
    "\n",
    "# Sample a few pairs to inspect\n",
    "print(\"\\nSample state pairs:\")\n",
    "for i, (source, target) in enumerate(state_pairs[:3]):\n",
    "    source_atoms = system.perceiver.step(source)\n",
    "    target_atoms = system.perceiver.step(target)\n",
    "    print(f\"  Pair {i}: {len(source_atoms)} source atoms -> {len(target_atoms)} target atoms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train Distance Heuristic with Custom DQN\n",
    "\n",
    "This is where the magic happens! The custom DQN will print detailed logs showing:\n",
    "- Episode statistics (reward, length, success rate)\n",
    "- Training metrics (loss, Q-values, TD errors)\n",
    "- Replay buffer size\n",
    "- Epsilon decay\n",
    "- Periodic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created heuristic config\n",
      "  Hidden sizes: None\n",
      "  HER k: 4\n"
     ]
    }
   ],
   "source": [
    "# Create distance heuristic config with custom DQN enabled\n",
    "heuristic_config = DistanceHeuristicConfig(\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=128,  # Smaller batch for tiny dataset\n",
    "    buffer_size=50000,  # Smaller buffer\n",
    "    max_episode_steps=MAX_EPISODE_STEPS,\n",
    "    learning_starts=LEARNING_STARTS,\n",
    "    device=device,\n",
    "    # Custom DQN specific\n",
    "    log_freq=LOG_FREQ,\n",
    "    train_freq=TRAIN_FREQ,\n",
    "    eval_freq=EVAL_FREQ,\n",
    "    target_update_freq=TARGET_UPDATE_FREQ,\n",
    "    her_k=4,\n",
    "    her_strategy=\"future\"\n",
    "    \n",
    ")\n",
    "\n",
    "print(\"✓ Created heuristic config\")\n",
    "print(f\"  Hidden sizes: {heuristic_config.hidden_sizes}\")\n",
    "print(f\"  HER k: {heuristic_config.her_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "\n",
      "Training distance heuristic V2 on 1920 state pairs...\n",
      "Device: cpu\n",
      "Computed observation statistics: mean=2.305, std=0.707\n",
      "\n",
      "================================================================================\n",
      "Using V2 self-contained DQN implementation\n",
      "================================================================================\n",
      "Observation dim: 12\n",
      "Goal dim: 12\n",
      "Number of actions: 5\n",
      "\n",
      "Starting training for 500000 steps...\n",
      "[DEBUG REWARD STEP 1] reward=-1.0\n",
      "[DEBUG REWARD STEP 2] reward=-1.0\n",
      "[DEBUG REWARD STEP 3] reward=-1.0\n",
      "[DEBUG REWARD STEP 4] reward=-1.0\n",
      "[DEBUG REWARD STEP 5] reward=-1.0\n",
      "[DEBUG REWARD STEP 6] reward=-1.0\n",
      "[DEBUG REWARD STEP 7] reward=-1.0\n",
      "[DEBUG REWARD STEP 8] reward=-1.0\n",
      "[DEBUG REWARD STEP 9] reward=-1.0\n",
      "[DEBUG REWARD STEP 10] reward=-1.0\n",
      "[DEBUG EPISODE 1] length=50, total_reward=-50.0, success=False\n",
      "[DEBUG EPISODE 2] length=1, total_reward=-1.0, success=False\n",
      "[DEBUG EPISODE 3] length=50, total_reward=-50.0, success=False\n",
      "\n",
      "[Step 1500]\n",
      "  Epsilon: 0.858\n",
      "  Replay buffer size: 1644\n",
      "  Avg Q-value: 0.637\n",
      "Loss: 0.8171060085296631\n",
      "\n",
      "[Step 22000]\n",
      "  Epsilon: 0.050\n",
      "  Replay buffer size: 24180\n",
      "  Avg Q-value: 0.097\n",
      "\n",
      "[Step 22500]\n",
      "  Epsilon: 0.050\n",
      "  Replay buffer size: 24720\n",
      "  Avg Q-value: 0.112\n",
      "\n",
      "[Evaluation at step 22500]\n",
      "  Avg episode length: 45.10\n",
      "  Avg episode reward: -45.10\n",
      "  Success rate: 0/10\n",
      "Loss: 0.2115378975868225\n",
      "\n",
      "[Step 23000]\n",
      "  Epsilon: 0.050\n",
      "  Replay buffer size: 25260\n",
      "  Avg Q-value: -0.056\n",
      "\n",
      "Training complete!\n",
      "\n",
      "✓ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Create and train heuristic\n",
    "heuristic = GoalConditionedDistanceHeuristicV2(config=heuristic_config, seed=SEED)\n",
    "\n",
    "print(\"\\nStarting training...\\n\")\n",
    "heuristic.train(\n",
    "    env=system.env,\n",
    "    state_pairs=state_pairs,\n",
    "    perceiver=system.perceiver,\n",
    "    max_training_steps=TRAINING_STEPS,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = heuristic.replay_buffer.transitions[77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.8309517\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(t['obs'] - t['desired_goal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t['reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "55\n",
      "56\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    t = heuristic.replay_buffer.transitions[-i]\n",
    "    if np.linalg.norm(t['obs'] - t['desired_goal']) == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.], dtype=float32)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_reward(t['achieved_goal'], t['desired_goal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.94774\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([t['reward'] for t in heuristic.replay_buffer.transitions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t['achieved_goal'].reshape(1, -1)\n",
    "d = t['desired_goal'].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.], dtype=float32)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_reward(a, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GoalConditionedDistanceHeuristicV2' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[334]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mheuristic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m.replay_buffer\n",
      "\u001b[31mAttributeError\u001b[39m: 'GoalConditionedDistanceHeuristicV2' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "heuristic.model.replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Distance Heuristic\n",
    "\n",
    "Compare learned distances f(s,s') with:\n",
    "1. True optimal distances (from rollouts)\n",
    "2. Graph distances (from planning graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing graph distances...\n",
      "✓ Computed 14 pairwise distances\n"
     ]
    }
   ],
   "source": [
    "# Compute graph distances\n",
    "print(\"Computing graph distances...\")\n",
    "graph_distances = compute_graph_distances(planning_graph, exclude_shortcuts=True)\n",
    "print(f\"✓ Computed {len(graph_distances)} pairwise distances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = state_pairs[3]\n",
    "heuristic.estimate_distance(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(GraphInstance(nodes=array([[0., 0., 0., 0., 0., 0.],\n",
       "        [1., 6., 8., 1., 1., 1.]], dtype=float32), edges=None, edge_links=None),\n",
       " {(InCol0 robot0), (InRow1 robot0)})"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, system.perceiver.step(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_true_distance(system, a, system.perceiver.step(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating learned distances...\n",
      "  Evaluated 10/1920 pairs...\n",
      "  Evaluated 20/1920 pairs...\n",
      "  Evaluated 30/1920 pairs...\n",
      "  Evaluated 40/1920 pairs...\n",
      "  Evaluated 50/1920 pairs...\n",
      "  Evaluated 60/1920 pairs...\n",
      "  Evaluated 70/1920 pairs...\n",
      "  Evaluated 80/1920 pairs...\n",
      "  Evaluated 90/1920 pairs...\n",
      "  Evaluated 100/1920 pairs...\n",
      "\n",
      "✓ Evaluated 100 state pairs\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on all state pairs\n",
    "from tamp_improv.approaches.improvisational.analyze import compute_true_distance\n",
    "import random\n",
    "\n",
    "print(\"\\nEvaluating learned distances...\")\n",
    "results = []\n",
    "\n",
    "sampled_state_pairs = [random.choice(state_pairs) for i in range(100)]\n",
    "for i, (source_state, target_state) in enumerate(sampled_state_pairs):\n",
    "    # Get learned distance\n",
    "    learned_dist = heuristic.estimate_distance(source_state, target_state)\n",
    "    \n",
    "    # Get true distance (from rollouts)\n",
    "    target_atoms = system.perceiver.step(target_state)\n",
    "    true_dist = compute_true_distance(system, source_state, target_atoms)\n",
    "    \n",
    "    # Get graph distance (approximation)\n",
    "    # Find which nodes these states belong to\n",
    "    source_atoms = system.perceiver.step(source_state)\n",
    "    source_node_id = None\n",
    "    target_node_id = None\n",
    "    \n",
    "    for node in planning_graph.nodes:\n",
    "        if node.atoms == source_atoms:\n",
    "            source_node_id = node.id\n",
    "        if node.atoms == target_atoms:\n",
    "            target_node_id = node.id\n",
    "    \n",
    "    if source_node_id is not None and target_node_id is not None:\n",
    "        graph_dist = graph_distances.get((source_node_id, target_node_id), float('inf'))\n",
    "    else:\n",
    "        graph_dist = float('inf')\n",
    "    \n",
    "    results.append({\n",
    "        'source_idx': i,\n",
    "        'learned_distance': learned_dist,\n",
    "        'true_distance': true_dist,\n",
    "        'graph_distance': graph_dist,\n",
    "    })\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Evaluated {i + 1}/{len(state_pairs)} pairs...\")\n",
    "\n",
    "print(f\"\\n✓ Evaluated {len(results)} state pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results Summary:\n",
      "  Total pairs: 100\n",
      "  Finite distance pairs: 51\n",
      "  Infinite distance pairs: 49\n"
     ]
    }
   ],
   "source": [
    "# Filter finite results\n",
    "finite_results = [r for r in results if r['graph_distance'] != float('inf')]\n",
    "infinite_results = [r for r in results if r['graph_distance'] == float('inf')]\n",
    "\n",
    "print(f\"\\nResults Summary:\")\n",
    "print(f\"  Total pairs: {len(results)}\")\n",
    "print(f\"  Finite distance pairs: {len(finite_results)}\")\n",
    "print(f\"  Infinite distance pairs: {len(infinite_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics (vs True Distance):\n",
      "  MAE: 4.24\n",
      "  RMSE: 5.38\n",
      "  Correlation: 0.174\n",
      "\n",
      "Statistics (vs Graph Distance):\n",
      "  MAE: 5.76\n",
      "  Correlation: 0.075\n",
      "\n",
      "Distance Ranges:\n",
      "  True:    [1.0, 13.0]\n",
      "  Graph:   [2.5, 17.2]\n",
      "  Learned: [1.4, 2.0]\n"
     ]
    }
   ],
   "source": [
    "# Compute statistics for finite distance pairs\n",
    "if finite_results:\n",
    "    true_dists = np.array([r['true_distance'] for r in finite_results])\n",
    "    learned_dists = np.array([r['learned_distance'] for r in finite_results])\n",
    "    graph_dists = np.array([r['graph_distance'] for r in finite_results])\n",
    "    \n",
    "    # vs True distance\n",
    "    mae_true = np.mean(np.abs(true_dists - learned_dists))\n",
    "    rmse_true = np.sqrt(np.mean((true_dists - learned_dists) ** 2))\n",
    "    correlation_true = np.corrcoef(true_dists, learned_dists)[0, 1]\n",
    "    \n",
    "    # vs Graph distance\n",
    "    mae_graph = np.mean(np.abs(graph_dists - learned_dists))\n",
    "    correlation_graph = np.corrcoef(graph_dists, learned_dists)[0, 1]\n",
    "    \n",
    "    print(f\"\\nStatistics (vs True Distance):\")\n",
    "    print(f\"  MAE: {mae_true:.2f}\")\n",
    "    print(f\"  RMSE: {rmse_true:.2f}\")\n",
    "    print(f\"  Correlation: {correlation_true:.3f}\")\n",
    "    \n",
    "    print(f\"\\nStatistics (vs Graph Distance):\")\n",
    "    print(f\"  MAE: {mae_graph:.2f}\")\n",
    "    print(f\"  Correlation: {correlation_graph:.3f}\")\n",
    "    \n",
    "    print(f\"\\nDistance Ranges:\")\n",
    "    print(f\"  True:    [{true_dists.min():.1f}, {true_dists.max():.1f}]\")\n",
    "    print(f\"  Graph:   [{graph_dists.min():.1f}, {graph_dists.max():.1f}]\")\n",
    "    print(f\"  Learned: [{learned_dists.min():.1f}, {learned_dists.max():.1f}]\")\n",
    "else:\n",
    "    print(\"\\nNo finite distance pairs to analyze!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Comparisons (sorted by true distance):\n",
      " Idx |   True |  Graph |  Learned |  Error\n",
      "---------------------------------------------\n",
      "  15 |    1.0 |    4.0 |      1.7 |    0.7\n",
      "  19 |    1.0 |    2.5 |      1.7 |    0.7\n",
      "  39 |    1.0 |    2.5 |      1.7 |    0.7\n",
      "  63 |    1.0 |    3.7 |      1.7 |    0.7\n",
      "  85 |    1.0 |    2.5 |      1.8 |    0.8\n",
      "  90 |    1.0 |    3.3 |      1.8 |    0.8\n",
      "  94 |    1.0 |    2.5 |      1.8 |    0.8\n",
      "  11 |    2.0 |    3.3 |      1.7 |    0.3\n",
      "  29 |    2.0 |   13.2 |      1.7 |    0.3\n",
      "  52 |    2.0 |    2.5 |      1.6 |    0.4\n",
      "  54 |    2.0 |    3.3 |      1.6 |    0.4\n",
      "  95 |    2.0 |    3.3 |      1.6 |    0.4\n",
      "  28 |    3.0 |    3.7 |      1.6 |    1.4\n",
      "  48 |    3.0 |    3.7 |      1.7 |    1.3\n",
      "  53 |    3.0 |    3.7 |      1.5 |    1.5\n",
      "  59 |    3.0 |    2.5 |      1.9 |    1.1\n",
      "   9 |    4.0 |    2.5 |      1.7 |    2.3\n",
      "  24 |    4.0 |    3.3 |      1.4 |    2.6\n",
      "  30 |    4.0 |   13.2 |      1.7 |    2.3\n",
      "  66 |    4.0 |    3.7 |      1.7 |    2.3\n"
     ]
    }
   ],
   "source": [
    "# Show sample comparisons\n",
    "if finite_results:\n",
    "    print(\"\\nSample Comparisons (sorted by true distance):\")\n",
    "    print(f\"{'Idx':>4} | {'True':>6} | {'Graph':>6} | {'Learned':>8} | {'Error':>6}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    sorted_results = sorted(finite_results, key=lambda r: r['true_distance'])\n",
    "    for r in sorted_results[:20]:  # Show first 20\n",
    "        error = abs(r['true_distance'] - r['learned_distance'])\n",
    "        print(\n",
    "            f\"{r['source_idx']:>4} | \"\n",
    "            f\"{r['true_distance']:>6.1f} | \"\n",
    "            f\"{r['graph_distance']:>6.1f} | \"\n",
    "            f\"{r['learned_distance']:>8.1f} | \"\n",
    "            f\"{error:>6.1f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(GraphInstance(nodes=array([[0., 5., 1., 1., 0., 0.],\n",
       "        [1., 5., 5., 1., 1., 1.]], dtype=float32), edges=None, edge_links=None),\n",
       " GraphInstance(nodes=array([[0., 5., 9., 1., 1., 0.],\n",
       "        [1., 5., 9., 1., 1., 1.]], dtype=float32), edges=None, edge_links=None))"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_state_pairs[68]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if finite_results:\n",
    "    true_dists = np.array([r['true_distance'] for r in finite_results])\n",
    "    learned_dists = np.array([r['learned_distance'] for r in finite_results])\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(true_dists, learned_dists, alpha=0.6)\n",
    "    plt.plot([true_dists.min(), true_dists.max()], \n",
    "             [true_dists.min(), true_dists.max()], \n",
    "             'r--', label='Perfect correlation')\n",
    "    plt.xlabel('True Distance')\n",
    "    plt.ylabel('Learned Distance')\n",
    "    plt.title(f'Learned vs True Distance\\n(Correlation: {correlation_true:.3f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    errors = true_dists - learned_dists\n",
    "    plt.hist(errors, bins=20, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Error (True - Learned)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Error Distribution\\n(MAE: {mae_true:.2f}, RMSE: {rmse_true:.2f})')\n",
    "    plt.axvline(0, color='r', linestyle='--', label='Zero error')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results to visualize!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Heuristic (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained heuristic\n",
    "save_path = Path.cwd() / \"outputs\" / \"custom_dqn_test\"\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "heuristic.save(str(save_path / \"distance_heuristic\"))\n",
    "print(f\"\\n✓ Saved heuristic to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### If the heuristic is learning well:\n",
    "- Increase training steps\n",
    "- Try larger environments (more cells, more teleporters)\n",
    "- Test on real benchmarks (obstacle2d, tower, etc.)\n",
    "\n",
    "### If the heuristic is not learning:\n",
    "- Check the training logs above - are Q-values decreasing?\n",
    "- Check success rate - is it improving over time?\n",
    "- Try different hyperparameters:\n",
    "  - Increase `custom_dqn_her_k` (more hindsight goals)\n",
    "  - Decrease `learning_starts` (start learning sooner)\n",
    "  - Increase `buffer_size` (more experience)\n",
    "  - Adjust `custom_dqn_target_update_freq`\n",
    "- Check state pairs - are they feasible?\n",
    "- Add more logging in custom_dqn.py to debug specific issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slap_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
