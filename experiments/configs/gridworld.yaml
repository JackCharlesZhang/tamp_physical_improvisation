env_name: GridworldTAMPSystem
render_mode: null
action_scale: 1.0

num_cells: 10
num_states_per_cell: 10
num_teleporters: 1

seed: 42
debug: false
num_episodes: 50
max_steps: 50
max_training_steps_per_shortcut: 50
collect_episodes: 5
episodes_per_scenario: 200
force_collect: false
force_train_policy: false
force_train_heuristic: false
force_prune: false
render: false
training_record_interval: 10
batch_size: 32

# Pruning method: "none", "rollouts", or "distance_heuristic"
pruning_method: rollouts

use_random_rollouts: true
num_rollouts_per_node: 1000
max_steps_per_rollout: 100
shortcut_success_threshold: 1

learning_rate: 3.0e-4
rl_batch_size: 32
n_epochs: 10
gamma: 0.99
ent_coef: 0.01
deterministic: false
early_stopping: true
fast_eval: false

training_data_dir: training_data/multi_rl
save_dir: trained_policies/multi_rl
policy_path: trained_policies/multi_rl/GraphObstacle2DTAMPSystem_MultiRL

# Pipeline cache directory (can be overridden via command line)
# Example: pipeline_cache_dir=/scratch/.../pipeline_cache/obstacle2d
pipeline_cache_dir: null

# Distance heuristic parameters (increased for better learning)
# Note: training_pairs * max_steps should be < training_steps
heuristic_collect_episodes: 5
heuristic_training_pairs: 100
heuristic_training_steps: 100000
heuristic_learning_rate: 3.0e-4
heuristic_batch_size: 256
heuristic_buffer_size: 100000
heuristic_max_steps: 200
heuristic_practical_horizon: 100  # K value for pruning: keep if f < min(D, K). Set to 2x training steps.