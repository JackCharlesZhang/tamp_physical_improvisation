{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from slap_train_pipeline_v2 import SerializableResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using run directory: /n/fs/jborz/projects/slap/outputs/2025-12-14/20-06-34\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "# Set specific run timestamp (e.g. \"2025-12-14/17-21-03\") or None to use most recent\n",
    "TARGET_RUN = None \n",
    "BASE_OUTPUT_DIR = \"/n/fs/jborz/projects/slap/outputs\"\n",
    "\n",
    "def get_run_dir(target_run=None, base_dir=BASE_OUTPUT_DIR):\n",
    "    \"\"\"Get the run directory, defaulting to most recent if not specified.\"\"\"\n",
    "    if target_run:\n",
    "        # Check if full path or relative path\n",
    "        if os.path.isabs(target_run):\n",
    "            return target_run\n",
    "        return os.path.join(base_dir, target_run)\n",
    "    \n",
    "    # Find most recent run\n",
    "    # Pattern: base_dir/YYYY-MM-DD/HH-MM-SS\n",
    "    date_dirs = sorted(glob.glob(os.path.join(base_dir, \"*-*-*\")))\n",
    "    if not date_dirs:\n",
    "        raise ValueError(f\"No date directories found in {base_dir}\")\n",
    "        \n",
    "    # Search latest date dir first\n",
    "    for date_dir in reversed(date_dirs):\n",
    "        time_dirs = sorted(glob.glob(os.path.join(date_dir, \"*-*-*\")))\n",
    "        if time_dirs:\n",
    "            return time_dirs[-1]\n",
    "            \n",
    "    raise ValueError(f\"No run directories found in {base_dir}\")\n",
    "\n",
    "run_dir = get_run_dir(TARGET_RUN)\n",
    "print(f\"Using run directory: {run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(run_dir + \"/results.pkl\", \"rb\") as f:\n",
    "    results: SerializableResults = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 0), (2, 1)), ((0, 1), (1, 2))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.teleporter_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing 2 teleporters...\n",
      "Teleporter (Coords)            Node IDs        In Train?  In Prune?  In Final?\n",
      "--------------------------------------------------------------------------------\n",
      "(1, 0) <-> (2, 1)              1 <-> 2         True       True       False\n",
      "(0, 1) <-> (1, 2)              4 <-> 5         True       True       True\n"
     ]
    }
   ],
   "source": [
    "def analyze_teleporters(results):\n",
    "    if not results.teleporter_locations:\n",
    "        print(\"No teleporter locations found.\")\n",
    "        return\n",
    "\n",
    "    # 1) Create mapping from (col, row) -> node_id using training data\n",
    "    data = results.final_training_data or results.training_data\n",
    "    if not data:\n",
    "        print(\"No training data found (final_training_data or training_data is missing).\")\n",
    "        return\n",
    "\n",
    "    node_atoms = data.node_atoms\n",
    "    coord_to_node = {}\n",
    "\n",
    "    for node_id, atoms in node_atoms.items():\n",
    "        col = -1\n",
    "        row = -1\n",
    "\n",
    "        for atom in atoms:\n",
    "            atom_str = str(atom)\n",
    "\n",
    "            # Match InColX and InRowY\n",
    "            col_match = re.search(r\"InCol(\\d+)\", atom_str)\n",
    "            if col_match:\n",
    "                col = int(col_match.group(1))\n",
    "\n",
    "            row_match = re.search(r\"InRow(\\d+)\", atom_str)\n",
    "            if row_match:\n",
    "                row = int(row_match.group(1))\n",
    "\n",
    "        if col != -1 and row != -1:\n",
    "            coord_to_node[(col, row)] = node_id\n",
    "\n",
    "    # 2) Get shortcut sets\n",
    "    if results.training_data:\n",
    "        unique_shortcuts_set = set(results.training_data.unique_shortcuts)\n",
    "    elif results.pruned_training_data:  # Fallback\n",
    "        unique_shortcuts_set = set(results.pruned_training_data.unique_shortcuts)\n",
    "    else:\n",
    "        unique_shortcuts_set = set()\n",
    "\n",
    "    pruned_set = set()\n",
    "    if results.pruned_training_data:\n",
    "        pruned_set = set(results.pruned_training_data.unique_shortcuts)\n",
    "\n",
    "    final_shortcuts_set = set()\n",
    "    if results.final_training_data:\n",
    "        final_shortcuts_set = set(results.final_training_data.unique_shortcuts)\n",
    "\n",
    "    # 3) Print table\n",
    "    print(f\"\\nAnalyzing {len(results.teleporter_locations)} teleporters...\")\n",
    "    print(\n",
    "        f\"{'Teleporter (Coords)':<30} {'Node IDs':<15} \"\n",
    "        f\"{'In Train?':<10} {'In Prune?':<10} {'In Final?'}\"\n",
    "    )\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for loc1, loc2 in results.teleporter_locations:\n",
    "        loc1 = tuple(loc1)\n",
    "        loc2 = tuple(loc2)\n",
    "\n",
    "        id1 = coord_to_node.get(loc1)\n",
    "        id2 = coord_to_node.get(loc2)\n",
    "\n",
    "        coords_str = f\"{loc1} <-> {loc2}\"\n",
    "\n",
    "        if id1 is None or id2 is None:\n",
    "            print(f\"{coords_str:<30} {'Not Found':<15} {'-':<10} {'-':<10} {'-'}\")\n",
    "            continue\n",
    "\n",
    "        in_train = ((id1, id2) in unique_shortcuts_set) or ((id2, id1) in unique_shortcuts_set)\n",
    "        in_prune = ((id1, id2) in pruned_set) or ((id2, id1) in pruned_set)\n",
    "        in_final = ((id1, id2) in final_shortcuts_set) or ((id2, id1) in final_shortcuts_set)\n",
    "\n",
    "        nodes_str = f\"{id1} <-> {id2}\"\n",
    "        print(f\"{coords_str:<30} {nodes_str:<15} {str(in_train):<10} {str(in_prune):<10} {str(in_final)}\")\n",
    "\n",
    "\n",
    "analyze_teleporters(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'initial_node': 0,\n",
       " 'goal_nodes': [5],\n",
       " 'optimal_path_nodes': [0, 1, 5],\n",
       " 'optimal_path_edges': [{'source': 0,\n",
       "   'target': 1,\n",
       "   'is_shortcut': False,\n",
       "   'cost': 4},\n",
       "  {'source': 1, 'target': 5, 'is_shortcut': False, 'cost': inf}],\n",
       " 'shortcuts_added': 7,\n",
       " 'success': True,\n",
       " 'true_steps': 13,\n",
       " 'reward': 88.0}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.evaluation_results.episode_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for shortcuts in optimal paths:\n",
      "  No shortcuts found in any optimal path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_episodes_with_shortcuts(episode_data: list[dict]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Identifies and prints episodes where a shortcut was used in the optimal path.\n",
    "\n",
    "    Args:\n",
    "        episode_data: A list of dictionaries, where each dictionary\n",
    "                      represents an episode and contains 'optimal_path_edges'.\n",
    "\n",
    "    Returns:\n",
    "        A list of episode indices where shortcuts were found.\n",
    "    \"\"\"\n",
    "    episodes_with_shortcuts = []\n",
    "    print(\"Checking for shortcuts in optimal paths:\")\n",
    "    for i, episode in enumerate(episode_data):\n",
    "        optimal_path_edges = episode.get('optimal_path_edges', [])\n",
    "        for edge in optimal_path_edges:\n",
    "            if edge.get('is_shortcut', False):\n",
    "                print(f\"  Shortcut used in Episode {i}\")\n",
    "                episodes_with_shortcuts.append(i)\n",
    "                break  # Only need to find one shortcut per episode\n",
    "    \n",
    "    if not episodes_with_shortcuts:\n",
    "        print(\"  No shortcuts found in any optimal path.\")\n",
    "\n",
    "    return episodes_with_shortcuts\n",
    "\n",
    "get_episodes_with_shortcuts(results.evaluation_results.episode_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
